{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7270572,"sourceType":"datasetVersion","datasetId":4214656},{"sourceId":7274383,"sourceType":"datasetVersion","datasetId":4217247}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\nimport torch\nfrom torch.nn.utils import spectral_norm\nimport argparse\nimport numpy as np\nimport cv2\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import Dataset\nimport torch.optim as optim\nimport os\nfrom tqdm import tqdm\nimport torchvision.models as models\nfrom torch.nn.parallel import DataParallel","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-05T21:41:52.777997Z","iopub.execute_input":"2024-01-05T21:41:52.778256Z","iopub.status.idle":"2024-01-05T21:41:56.799851Z","shell.execute_reply.started":"2024-01-05T21:41:52.778231Z","shell.execute_reply":"2024-01-05T21:41:56.798986Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Dataset","metadata":{}},{"cell_type":"code","source":"class AnimeDataSet(Dataset):\n    def __init__(self, args, transform=None):\n        \"\"\"\n        folder structure:\n            - {data_dir}\n                - photo\n                    1.jpg, ..., n.jpg\n                - {dataset}  # E.g Hayao\n                    smooth\n                        1.jpg, ..., n.jpg\n                    style\n                        1.jpg, ..., n.jpg\n        \"\"\"\n        data_dir = args['data_dir']\n        dataset = args['dataset']\n        # data_dir = \"data\"\n        # dataset = \"Hayao\"\n\n        anime_dir = os.path.join(data_dir, dataset)\n        if not os.path.exists(data_dir):\n            raise FileNotFoundError(f'Folder {data_dir} does not exist')\n\n        if not os.path.exists(anime_dir):\n            raise FileNotFoundError(f'Folder {anime_dir} does not exist')\n\n        self.mean = compute_data_mean(os.path.join(anime_dir, 'style'))\n        print(f'Mean(B, G, R) of {dataset} are {self.mean}')\n\n        self.data_dir = data_dir\n        self.image_files = {}\n        self.photo = 'train_photo'\n        self.style = f'{dataset}/style'\n        self.smooth = f'{dataset}/smooth'\n        self.dummy = torch.zeros(3, 256, 256)\n\n        for opt in [self.photo, self.style, self.smooth]:\n            folder = os.path.join(data_dir, opt)\n            files = os.listdir(folder)\n            self.image_files[opt] = [os.path.join(folder, fi) for fi in files]\n\n        self.transform = transform\n        print(f'Dataset: real {len(self.image_files[self.photo])} style {self.len_anime}, smooth {self.len_smooth}')\n\n    def __len__(self):\n        return len(self.image_files[self.photo])\n\n    @property\n    def len_anime(self):\n        return len(self.image_files[self.style])\n\n    @property\n    def len_smooth(self):\n        return len(self.image_files[self.smooth])\n\n    def __getitem__(self, index):\n        image = self.load_photo(index)\n        anm_idx = index\n        if anm_idx > self.len_anime - 1:\n            anm_idx -= self.len_anime * (index // self.len_anime)\n\n        anime, anime_gray = self.load_anime(anm_idx)\n        smooth_gray = self.load_anime_smooth(anm_idx)\n\n        return image, anime, anime_gray, smooth_gray\n\n    def load_photo(self, index):\n        fpath = self.image_files[self.photo][index]\n        image = cv2.imread(fpath)[:,:,::-1]\n        image = self._transform(image, addmean=False)\n        image = image.transpose(2, 0, 1)\n        return torch.tensor(image)\n\n    def load_anime(self, index):\n        fpath = self.image_files[self.style][index]\n        image = cv2.imread(fpath)[:,:,::-1]\n\n        image_gray = cv2.cvtColor(image.copy(), cv2.COLOR_BGR2GRAY)\n        image_gray = np.stack([image_gray, image_gray, image_gray], axis=-1)\n        image_gray = self._transform(image_gray, addmean=False)\n        image_gray = image_gray.transpose(2, 0, 1)\n\n        image = self._transform(image, addmean=True)\n        image = image.transpose(2, 0, 1)\n\n        return torch.tensor(image), torch.tensor(image_gray)\n\n    def load_anime_smooth(self, index):\n        fpath = self.image_files[self.smooth][index]\n        image = cv2.imread(fpath, cv2.IMREAD_GRAYSCALE)\n        image = np.stack([image, image, image], axis=-1)\n        image = self._transform(image, addmean=False)\n        image = image.transpose(2, 0, 1)\n        return torch.tensor(image)\n\n    def _transform(self, img, addmean=True):\n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n\n        img = img.astype(np.float32)\n        if addmean:\n            img += self.mean\n        return normalize_input(img)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:41.823138Z","iopub.execute_input":"2024-01-05T21:42:41.823637Z","iopub.status.idle":"2024-01-05T21:42:41.844278Z","shell.execute_reply.started":"2024-01-05T21:42:41.823605Z","shell.execute_reply":"2024-01-05T21:42:41.843372Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Model Architecture","metadata":{}},{"cell_type":"code","source":"\ndef initialize_weights(net):\n    for m in net.modules():\n        try:\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.ConvTranspose2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        except Exception as e:\n            # print(f'SKip layer {m}, {e}')\n            pass","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:42.461174Z","iopub.execute_input":"2024-01-05T21:42:42.461730Z","iopub.status.idle":"2024-01-05T21:42:42.468546Z","shell.execute_reply.started":"2024-01-05T21:42:42.461700Z","shell.execute_reply":"2024-01-05T21:42:42.467610Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class ConvBlock(nn.Module):\n    def __init__(self, channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False):\n        super(ConvBlock, self).__init__()\n        self.conv = nn.Conv2d(channels, out_channels,\n            kernel_size=kernel_size, stride=stride, padding=padding, bias=bias)\n        self.ins_norm = nn.InstanceNorm2d(out_channels)\n        self.activation = nn.LeakyReLU(0.2, True)\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.ins_norm(out)\n        out = self.activation(out)\n        return out\n\nclass SeparableConv2D(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1, bias=False):\n        super(SeparableConv2D, self).__init__()\n        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3,\n            stride=stride, padding=1, groups=in_channels, bias=bias)\n        self.pointwise = nn.Conv2d(in_channels, out_channels,\n            kernel_size=1, stride=1, bias=bias)\n        # self.pad =\n        self.ins_norm1 = nn.InstanceNorm2d(in_channels)\n        self.activation1 = nn.LeakyReLU(0.2, True)\n        self.ins_norm2 = nn.InstanceNorm2d(out_channels)\n        self.activation2 = nn.LeakyReLU(0.2, True)\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.ins_norm1(out)\n        out = self.activation1(out)\n        out = self.pointwise(out)\n        out = self.ins_norm2(out)\n        return self.activation2(out)\n\nclass DownConv(nn.Module):\n    def __init__(self, channels, bias=False):\n        super(DownConv, self).__init__()\n        self.conv1 = SeparableConv2D(channels, channels, stride=2, bias=bias)\n        self.conv2 = SeparableConv2D(channels, channels, stride=1, bias=bias)\n\n    def forward(self, x):\n        out1 = self.conv1(x)\n        out2 = F.interpolate(x, scale_factor=0.5, mode='bilinear')\n        out2 = self.conv2(out2)\n        return out1 + out2\n\nclass UpConv(nn.Module):\n    def __init__(self, channels, bias=False):\n        super(UpConv, self).__init__()\n        self.conv = SeparableConv2D(channels, channels, stride=1, bias=bias)\n\n    def forward(self, x):\n        out = F.interpolate(x, scale_factor=2.0, mode='bilinear')\n        out = self.conv(out)\n        return out\n\nclass InvertedResBlock(nn.Module):\n    def __init__(self, channels=256, out_channels=256, expand_ratio=2, bias=False):\n        super(InvertedResBlock, self).__init__()\n        bottleneck_dim = round(expand_ratio * channels)\n        self.conv_block = ConvBlock(channels, bottleneck_dim, kernel_size=1, stride=1, padding=0, bias=bias)\n        self.depthwise_conv = nn.Conv2d(bottleneck_dim, bottleneck_dim,\n            kernel_size=3, groups=bottleneck_dim, stride=1, padding=1, bias=bias)\n        self.conv = nn.Conv2d(bottleneck_dim, out_channels,\n            kernel_size=1, stride=1, bias=bias)\n        self.ins_norm1 = nn.InstanceNorm2d(bottleneck_dim)\n        self.ins_norm2 = nn.InstanceNorm2d(out_channels)\n        self.activation = nn.LeakyReLU(0.2, True)\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.conv_block(x)\n        out = self.depthwise_conv(out)\n        out = self.ins_norm1(out)\n        out = self.activation(out)\n        out = self.conv(out)\n        out = self.ins_norm2(out)\n        return out + x","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:42.821139Z","iopub.execute_input":"2024-01-05T21:42:42.821864Z","iopub.status.idle":"2024-01-05T21:42:42.840829Z","shell.execute_reply.started":"2024-01-05T21:42:42.821833Z","shell.execute_reply":"2024-01-05T21:42:42.839970Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, dataset=''):\n        super(Generator, self).__init__()\n        self.name = f'generator_{dataset}'\n        bias = False\n\n        self.encode_blocks = nn.Sequential(\n            ConvBlock(3, 64, bias=bias),\n            ConvBlock(64, 128, bias=bias),\n            DownConv(128, bias=bias),\n            ConvBlock(128, 128, bias=bias),\n            SeparableConv2D(128, 256, bias=bias),\n            DownConv(256, bias=bias),\n            ConvBlock(256, 256, bias=bias),\n        )\n\n        self.res_blocks = nn.Sequential(\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n            InvertedResBlock(256, 256, bias=bias),\n        )\n\n        self.decode_blocks = nn.Sequential(\n            ConvBlock(256, 128, bias=bias),\n            UpConv(128, bias=bias),\n            SeparableConv2D(128, 128, bias=bias),\n            ConvBlock(128, 128, bias=bias),\n            UpConv(128, bias=bias),\n            ConvBlock(128, 64, bias=bias),\n            ConvBlock(64, 64, bias=bias),\n            nn.Conv2d(64, 3, kernel_size=1, stride=1, padding=0, bias=bias),\n            nn.Tanh(),\n        )\n        initialize_weights(self)\n\n    def forward(self, x):\n        out = self.encode_blocks(x)\n        out = self.res_blocks(out)\n        img = self.decode_blocks(out)\n        return img","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:43.199700Z","iopub.execute_input":"2024-01-05T21:42:43.199972Z","iopub.status.idle":"2024-01-05T21:42:43.210553Z","shell.execute_reply.started":"2024-01-05T21:42:43.199949Z","shell.execute_reply":"2024-01-05T21:42:43.209595Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, dataset, args):\n        super(Discriminator, self).__init__()\n        self.name = f\"discriminator_{dataset}\"\n        self.bias = False\n        channels = 32\n\n        layers = [\n            nn.Conv2d(3, channels, kernel_size=3, stride=1, padding=1, bias=self.bias),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        for i in range(args['d_layers']):\n            layers += [\n                nn.Conv2d(channels, channels * 2, kernel_size=3, stride=2, padding=1, bias=self.bias),\n                nn.LeakyReLU(0.2, True),\n                nn.Conv2d(channels * 2, channels * 4, kernel_size=3, stride=1, padding=1, bias=self.bias),\n                nn.InstanceNorm2d(channels * 4),\n                nn.LeakyReLU(0.2, True),\n            ]\n            channels *= 4\n\n        layers += [\n            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=self.bias),\n            nn.InstanceNorm2d(channels),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(channels, 1, kernel_size=3, stride=1, padding=1, bias=self.bias),\n        ]\n\n        if args['use_sn']:\n            for i in range(len(layers)):\n                if isinstance(layers[i], nn.Conv2d):\n                    layers[i] = spectral_norm(layers[i])\n\n        self.discriminate = nn.Sequential(*layers)\n        initialize_weights(self)\n\n    def forward(self, img):\n        return self.discriminate(img)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:45.962414Z","iopub.execute_input":"2024-01-05T21:42:45.963023Z","iopub.status.idle":"2024-01-05T21:42:45.973423Z","shell.execute_reply.started":"2024-01-05T21:42:45.962977Z","shell.execute_reply":"2024-01-05T21:42:45.972517Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"vgg_mean = torch.tensor([0.485, 0.456, 0.406]).float()\nvgg_std = torch.tensor([0.229, 0.224, 0.225]).float()\n\nif torch.cuda.is_available():\n    vgg_std = vgg_std.cuda()\n    vgg_mean = vgg_mean.cuda()\n\nclass Vgg19(nn.Module):\n    def __init__(self):\n        super(Vgg19, self).__init__()\n        self.vgg19 = self.get_vgg19().eval()\n        self.mean = vgg_mean.view(-1, 1 ,1)\n        self.std = vgg_std.view(-1, 1, 1)\n\n    def forward(self, x):\n        return self.vgg19(self.normalize_vgg(x))\n\n    @staticmethod\n    def get_vgg19(last_layer='conv4_4'):\n        vgg = models.vgg19(weights='IMAGENET1K_V1').features\n        model_list = []\n        i = 0\n        j = 1\n        for layer in vgg.children():\n            if isinstance(layer, nn.MaxPool2d):\n                i = 0\n                j += 1\n            elif isinstance(layer, nn.Conv2d):\n                i += 1\n            name = f'conv{j}_{i}'\n            if name == last_layer:\n                model_list.append(layer)\n                break\n            model_list.append(layer)\n        model = nn.Sequential(*model_list)\n        return model\n\n\n    def normalize_vgg(self, image):\n        '''\n        Expect input in range -1 1\n        '''\n        image = (image + 1.0) / 2.0\n        return (image - self.mean) / self.std","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:47.231581Z","iopub.execute_input":"2024-01-05T21:42:47.232510Z","iopub.status.idle":"2024-01-05T21:42:47.431460Z","shell.execute_reply.started":"2024-01-05T21:42:47.232476Z","shell.execute_reply":"2024-01-05T21:42:47.430666Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Utils","metadata":{}},{"cell_type":"code","source":"import torch\nimport gc\nimport os\nimport torch.nn as nn\nimport urllib.request\nimport cv2\nfrom tqdm import tqdm\n\nHTTP_PREFIXES = [\n    'http',\n    'data:image/jpeg',\n]\n\nSUPPORT_WEIGHTS = {\n    'hayao',\n    'shinkai',\n}\n\nASSET_HOST = 'https://github.com/ptran1203/pytorch-animeGAN/releases/download/v1.0'\n\ndef read_image(path):\n    \"\"\"\n    Read image from given path\n    \"\"\"\n    if any(path.startswith(p) for p in HTTP_PREFIXES):\n        urllib.request.urlretrieve(path, \"temp.jpg\")\n        path = \"temp.jpg\"\n    return cv2.imread(path)[: ,: ,::-1]\n\n\ndef save_checkpoint(model, optimizer, epoch, args, posfix=''):\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'epoch': epoch,\n    }\n    path = os.path.join(args['checkpoint_dir'], f'{model.name}{posfix}.pth')\n    torch.save(checkpoint, path)\n\n\ndef load_checkpoint(model, optimizer, checkpoint_dir, posfix=''):\n    path = os.path.join(checkpoint_dir, f'{model.name}{posfix}.pth')\n    return load_weight(model, optimizer, path)\n\n\ndef load_weight(model, optimizer, weight):\n    if weight.lower() in SUPPORT_WEIGHTS:\n        weight = _download_weight(weight)\n\n    checkpoint = torch.load(weight,  map_location='cuda:0') if torch.cuda.is_available() else \\\n        torch.load(weight,  map_location='cpu')\n    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    epoch = checkpoint['epoch']\n    del checkpoint\n    torch.cuda.empty_cache()\n    gc.collect()\n    return epoch\n\ndef initialize_weights(net):\n    for m in net.modules():\n        try:\n            if isinstance(m, nn.Conv2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.ConvTranspose2d):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.02)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n        except Exception as e:\n            # print(f'SKip layer {m}, {e}')\n            pass\n\ndef set_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\nclass DownloadProgressBar(tqdm):\n    '''\n    https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads\n    '''\n    def update_to(self, b=1, bsize=1, tsize=None):\n        if tsize is not None:\n            self.total = tsize\n        self.update(b * bsize - self.n)\n\n\ndef _download_weight(weight):\n    '''\n    Download weight and save to local file\n    '''\n    filename = f'generator_{weight.lower()}.pth'\n    os.makedirs('.cache', exist_ok=True)\n    url = f'{ASSET_HOST}/{filename}'\n    save_path = f'.cache/{filename}'\n    if os.path.isfile(save_path):\n        return save_path\n    desc = f'Downloading {url} to {save_path}'\n    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=desc) as t:\n        urllib.request.urlretrieve(url, save_path, reporthook=t.update_to)\n    return save_path","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:49.881104Z","iopub.execute_input":"2024-01-05T21:42:49.881503Z","iopub.status.idle":"2024-01-05T21:42:49.900133Z","shell.execute_reply.started":"2024-01-05T21:42:49.881459Z","shell.execute_reply":"2024-01-05T21:42:49.899153Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch\nimport cv2\nimport os\nimport numpy as np\nfrom tqdm import tqdm\n\n_rgb_to_yuv_kernel = torch.tensor([\n    [0.299, -0.14714119, 0.61497538],\n    [0.587, -0.28886916, -0.51496512],\n    [0.114, 0.43601035, -0.10001026]\n]).float()\n\nif torch.cuda.is_available():\n    _rgb_to_yuv_kernel = _rgb_to_yuv_kernel.cuda()\n\ndef gram(input):\n    \"\"\"\n    Calculate Gram Matrix\n    https://pytorch.org/tutorials/advanced/neural_style_tutorial.html#style-loss\n    \"\"\"\n    b, c, w, h = input.size()\n    x = input.view(b * c, w * h)\n    G = torch.mm(x, x.T)\n    # normalize by total elements\n    return G.div(b * c * w * h)\n\ndef rgb_to_yuv(image):\n    '''\n    https://en.wikipedia.org/wiki/YUV\n    output: Image of shape (H, W, C) (channel last)\n    '''\n    # -1 1 -> 0 1\n    image = (image + 1.0) / 2.0\n    yuv_img = torch.tensordot(\n        image,\n        _rgb_to_yuv_kernel,\n        dims=([image.ndim - 3], [0]))\n    return yuv_img\n\n\ndef divisible(dim):\n    '''\n    Make width and height divisible by 32\n    '''\n    width, height = dim\n    return width - (width % 32), height - (height % 32)\n\ndef resize_image(image, width=None, height=None, inter=cv2.INTER_AREA):\n    dim = None\n    h, w = image.shape[:2]\n    if width and height:\n        return cv2.resize(image, divisible((width, height)),  interpolation=inter)\n    if width is None and height is None:\n        return cv2.resize(image, divisible((w, h)),  interpolation=inter)\n    if width is None:\n        r = height / float(h)\n        dim = (int(w * r), height)\n    else:\n        r = width / float(w)\n        dim = (width, int(h * r))\n    return cv2.resize(image, divisible(dim), interpolation=inter)\n\n\ndef normalize_input(images):\n    '''\n    [0, 255] -> [-1, 1]\n    '''\n    return images / 127.5 - 1.0\n\n\ndef denormalize_input(images, dtype=None):\n    '''\n    [-1, 1] -> [0, 255]\n    '''\n    images = images * 127.5 + 127.5\n    if dtype is not None:\n        if isinstance(images, torch.Tensor):\n            images = images.type(dtype)\n        else:\n            # numpy.ndarray\n            images = images.astype(dtype)\n\n    return images\n\n\ndef compute_data_mean(data_folder):\n    if not os.path.exists(data_folder):\n        raise FileNotFoundError(f'Folder {data_folder} does not exits')\n    image_files = os.listdir(data_folder)\n    total = np.zeros(3)\n    print(f\"Compute mean (R, G, B) from {len(image_files)} images\")\n    for img_file in tqdm(image_files):\n        path = os.path.join(data_folder, img_file)\n        image = cv2.imread(path)\n        total += image.mean(axis=(0, 1))\n    channel_mean = total / len(image_files)\n    mean = np.mean(channel_mean)\n    return mean - channel_mean[...,::-1]  # Convert to BGR for training","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:51.382207Z","iopub.execute_input":"2024-01-05T21:42:51.382836Z","iopub.status.idle":"2024-01-05T21:42:51.399730Z","shell.execute_reply.started":"2024-01-05T21:42:51.382804Z","shell.execute_reply":"2024-01-05T21:42:51.398836Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### Loss","metadata":{}},{"cell_type":"code","source":"class ColorLoss(nn.Module):\n    def __init__(self):\n        super(ColorLoss, self).__init__()\n        self.l1 = nn.L1Loss()\n        self.huber = nn.SmoothL1Loss()\n\n    def forward(self, image, image_g):\n        image = rgb_to_yuv(image)\n        image_g = rgb_to_yuv(image_g)\n\n        return (self.l1(image[:, :, :, 0], image_g[:, :, :, 0]) +\n                self.huber(image[:, :, :, 1], image_g[:, :, :, 1]) +\n                self.huber(image[:, :, :, 2], image_g[:, :, :, 2]))\n\n\nclass AnimeGanLoss:\n    def __init__(self, args):\n        self.content_loss = nn.L1Loss().to(args['device'])\n        self.gram_loss = nn.L1Loss().to(args['device'])\n        self.color_loss = ColorLoss().to(args['device'])\n        self.wadvg = args['wadvg']\n        self.wadvd = args['wadvd']\n        self.wcon = args['wcon']\n        self.wgra = args['wgra']\n        self.wcol = args['wcol']\n        self.vgg19 = Vgg19().to(args['device']).eval()\n        self.adv_type = args['gan_loss']\n        self.bce_loss = nn.BCELoss()\n\n    def compute_loss_G(self, fake_img, img, fake_logit, anime_gray):\n        '''\n        Compute loss for Generator\n\n        @Arugments:\n            - fake_img: generated image\n            - img: image\n            - fake_logit: output of Discriminator given fake image\n            - anime_gray: grayscale of anime image\n\n        @Returns:\n            loss\n        '''\n        fake_feat = self.vgg19(fake_img)\n        anime_feat = self.vgg19(anime_gray)\n        img_feat = self.vgg19(img).detach()\n\n        return [\n            self.wadvg * self.adv_loss_g(fake_logit),\n            self.wcon * self.content_loss(img_feat, fake_feat),\n            self.wgra * self.gram_loss(gram(anime_feat), gram(fake_feat)),\n            self.wcol * self.color_loss(img, fake_img),\n        ]\n\n    def compute_loss_D(self, fake_img_d, real_anime_d, real_anime_gray_d, real_anime_smooth_gray_d):\n        return self.wadvd * (\n            self.adv_loss_d_real(real_anime_d) +\n            self.adv_loss_d_fake(fake_img_d) +\n            self.adv_loss_d_fake(real_anime_gray_d) +\n            0.2 * self.adv_loss_d_fake(real_anime_smooth_gray_d)\n        )\n\n\n    def content_loss_vgg(self, image, recontruction):\n        feat = self.vgg19(image)\n        re_feat = self.vgg19(recontruction)\n\n        return self.content_loss(feat, re_feat)\n\n    def adv_loss_d_real(self, pred):\n        if self.adv_type == 'hinge':\n            return torch.mean(F.relu(1.0 - pred))\n\n        elif self.adv_type == 'lsgan':\n            return torch.mean(torch.square(pred - 1.0))\n\n        elif self.adv_type == 'normal':\n            return self.bce_loss(pred, torch.ones_like(pred))\n\n        raise ValueError(f'Do not support loss type {self.adv_type}')\n\n    def adv_loss_d_fake(self, pred):\n        if self.adv_type == 'hinge':\n            return torch.mean(F.relu(1.0 + pred))\n\n        elif self.adv_type == 'lsgan':\n            return torch.mean(torch.square(pred))\n\n        elif self.adv_type == 'normal':\n            return self.bce_loss(pred, torch.zeros_like(pred))\n\n        raise ValueError(f'Do not support loss type {self.adv_type}')\n\n\n    def adv_loss_g(self, pred):\n        if self.adv_type == 'hinge':\n            return -torch.mean(pred)\n\n        elif self.adv_type == 'lsgan':\n            return torch.mean(torch.square(pred - 1.0))\n\n        elif self.adv_type == 'normal':\n            return self.bce_loss(pred, torch.zeros_like(pred))\n\n        raise ValueError(f'Do not support loss type {self.adv_type}')\n\n\nclass LossSummary:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.loss_g_adv = []\n        self.loss_content = []\n        self.loss_gram = []\n        self.loss_color = []\n        self.loss_d_adv = []\n\n    def update_loss_G(self, adv, gram, color, content):\n        self.loss_g_adv.append(adv.cpu().detach().numpy())\n        self.loss_gram.append(gram.cpu().detach().numpy())\n        self.loss_color.append(color.cpu().detach().numpy())\n        self.loss_content.append(content.cpu().detach().numpy())\n\n    def update_loss_D(self, loss):\n        self.loss_d_adv.append(loss.cpu().detach().numpy())\n\n    def avg_loss_G(self):\n        return (\n            self._avg(self.loss_g_adv),\n            self._avg(self.loss_gram),\n            self._avg(self.loss_color),\n            self._avg(self.loss_content),\n        )\n\n    def avg_loss_D(self):\n        return self._avg(self.loss_d_adv)\n\n\n    @staticmethod\n    def _avg(losses):\n        return sum(losses) / len(losses)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:53.555781Z","iopub.execute_input":"2024-01-05T21:42:53.556555Z","iopub.status.idle":"2024-01-05T21:42:53.580382Z","shell.execute_reply.started":"2024-01-05T21:42:53.556520Z","shell.execute_reply":"2024-01-05T21:42:53.579525Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def gaussian_noise():\n    return torch.normal(torch.tensor(0.0), torch.tensor(0.1))\n\ndef checkDir(args):\n    if not os.path.exists(args['checkpoint_dir']):\n        print(f\"* {args['checkpoint_dir']} does not exist, creating...\")\n        os.makedirs(args['checkpoint_dir'])\n    if not os.path.exists(args['save_image_dir']):\n        print(f\"* {args['save_image_dir']} does not exist, creating...\")\n        os.makedirs(args['save_image_dir'])\n\ndef collate_fn(batch):\n    img, anime, anime_gray, anime_smt_gray = zip(*batch)\n    return (\n        torch.stack(img, 0),\n        torch.stack(anime, 0),\n        torch.stack(anime_gray, 0),\n        torch.stack(anime_smt_gray, 0),\n    )\n\ndef save_samples(generator, loader, args, max_imgs=2, subname='gen'):\n    '''\n    Generate and save images\n    '''\n    generator.eval()\n    max_iter = (max_imgs // args['batch_size']) + 1\n    fake_imgs = []\n    for i, (img, *_) in enumerate(loader):\n        with torch.no_grad():\n            fake_img = generator(img.cuda())\n            fake_img = fake_img.detach().cpu().numpy()\n            # Channel first -> channel last\n            fake_img = fake_img.transpose(0, 2, 3, 1)\n            fake_imgs.append(denormalize_input(fake_img, dtype=np.int16))\n        if i + 1 == max_iter:\n            break\n    fake_imgs = np.concatenate(fake_imgs, axis=0)\n    for i, img in enumerate(fake_imgs):\n        save_path = os.path.join(args['save_image_dir'], f'{subname}_{i}.jpg')\n        cv2.imwrite(save_path, img[..., ::-1])\n\ndef main(args):\n    checkDir(args)\n    G = Generator('Hayao').to(args['device'])\n#     G = DataParallel(G)\n    D = Discriminator('Hayao', args).to(args['device'])\n#     D = DataParallel(D)\n\n    loss_tracker = LossSummary()\n    loss_fn = AnimeGanLoss(args)\n    optimizer_g = optim.Adam(G.parameters(), lr=args['lr_g'], betas=(0.5, 0.999))\n    optimizer_d = optim.Adam(D.parameters(), lr=args['lr_d'], betas=(0.5, 0.999))\n\n    data_loader = DataLoader(\n        AnimeDataSet(args),\n        batch_size=args['batch_size'],\n        pin_memory=True,\n        shuffle=True,\n        collate_fn=collate_fn,\n    )\n\n    start_e = 0\n    if args['resume'] == 'GD':\n        # Load G and D\n        try:\n            start_e = load_checkpoint(G, optimizer_g, '/kaggle/working/checkpoints')+1\n            print(\"G weight loaded\")\n            load_checkpoint(D, optimizer_d, '/kaggle/working/checkpoints')\n            print(\"D weight loaded\")\n        except Exception as e:\n            print('Could not load checkpoint, train from scratch', e)\n    elif args['resume'] == 'G':\n        # Load G only\n        try:\n            start_e = load_checkpoint(G, optimizer_g, args['checkpoint_dir'], posfix='_init')+1\n        except Exception as e:\n            print('Could not load G init checkpoint, train from scratch', e)\n\n    for e in range(start_e, args['epochs']):\n        print(f\"Epoch {e}/{args['epochs']}\")\n        bar = tqdm(data_loader)\n        G.train()\n        init_losses = []\n        if e < args['init_epochs']:\n            # Train with content loss only\n            set_lr(optimizer_g, args['init_lr'])\n            for img, *_ in bar:\n                img = img.to(args['device'])\n                optimizer_g.zero_grad()\n\n                fake_img = G(img)\n                loss = loss_fn.content_loss_vgg(img, fake_img)\n                loss.backward()\n                optimizer_g.step()\n\n                init_losses.append(loss.cpu().detach().numpy())\n                avg_content_loss = sum(init_losses) / len(init_losses)\n                bar.set_description(f'[Init Training G] content loss: {avg_content_loss:2f}')\n\n            set_lr(optimizer_g, args['lr_g'])\n            save_checkpoint(G, optimizer_g, e, args, posfix='_init')\n            save_samples(G, data_loader, args, subname='initg')\n            continue\n\n        loss_tracker.reset()\n        for i, (img, anime, anime_gray, anime_smt_gray) in enumerate(bar):\n            img = img.to(args['device'])\n            anime = anime.to(args['device'])\n            anime_gray = anime_gray.to(args['device'])\n            anime_smt_gray = anime_smt_gray.to(args['device'])\n\n#             # ---------------- TRAIN D ---------------- #\n#             optimizer_d.zero_grad()\n#             fake_img = G(img).detach()\n#             # Add some Gaussian noise to images before feeding to D\n#             if args['d_noise']:\n#                 fake_img += gaussian_noise()\n#                 anime += gaussian_noise()\n#                 anime_gray += gaussian_noise()\n#                 anime_smt_gray += gaussian_noise()\n#             fake_d = D(fake_img)\n#             real_anime_d = D(anime)\n#             real_anime_gray_d = D(anime_gray)\n#             real_anime_smt_gray_d = D(anime_smt_gray)\n#             # Compute Loss\n#             loss_d = loss_fn.compute_loss_D(\n#                 fake_d, real_anime_d, real_anime_gray_d, real_anime_smt_gray_d)\n#             # Backward\n#             loss_d.backward()\n#             optimizer_d.step()\n#             loss_tracker.update_loss_D(loss_d)\n\n            # ---------------- TRAIN G ---------------- #\n            optimizer_g.zero_grad()\n            fake_img = G(img)\n            fake_d = D(fake_img)\n            # Compute Loss\n            adv_loss, con_loss, gra_loss, col_loss = loss_fn.compute_loss_G(\n                fake_img, img, fake_d, anime_gray)\n            loss_g = adv_loss + con_loss + gra_loss + col_loss\n            # Backward\n            loss_g.backward()\n            optimizer_g.step()\n            loss_tracker.update_loss_G(adv_loss, gra_loss, col_loss, con_loss)\n            \n            # ---------------- TRAIN D ---------------- #\n            if i%15 == 0:\n                optimizer_d.zero_grad()\n                fake_img = G(img).detach()\n                # Add some Gaussian noise to images before feeding to D\n                if args['d_noise']:\n                    fake_img += gaussian_noise()\n                    anime += gaussian_noise()\n                    anime_gray += gaussian_noise()\n                    anime_smt_gray += gaussian_noise()\n                fake_d = D(fake_img)\n                real_anime_d = D(anime)\n                real_anime_gray_d = D(anime_gray)\n                real_anime_smt_gray_d = D(anime_smt_gray)\n                # Compute Loss\n                loss_d = loss_fn.compute_loss_D(\n                    fake_d, real_anime_d, real_anime_gray_d, real_anime_smt_gray_d)\n                # Backward\n                loss_d.backward()\n                optimizer_d.step()\n                loss_tracker.update_loss_D(loss_d)\n\n            avg_adv, avg_gram, avg_color, avg_content = loss_tracker.avg_loss_G()\n            avg_adv_d = loss_tracker.avg_loss_D()\n            bar.set_description(f'loss G: adv {avg_adv:2f} con {avg_content:2f} gram {avg_gram:2f} color {avg_color:2f} / loss D: {avg_adv_d:2f}')\n            \n            \n        if e % args['save_interval'] == 0:\n            save_checkpoint(G, optimizer_g, e, args)\n            save_checkpoint(D, optimizer_d, e, args)\n            save_samples(G, data_loader, args)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:55.039636Z","iopub.execute_input":"2024-01-05T21:42:55.039963Z","iopub.status.idle":"2024-01-05T21:42:55.068640Z","shell.execute_reply.started":"2024-01-05T21:42:55.039938Z","shell.execute_reply":"2024-01-05T21:42:55.067714Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"json_config = {\n  \"dataset\": \"/kaggle/input/animedataset/Hayao\",\n  \"data_dir\": \"/kaggle/input/animedataset\",\n  \"epochs\": 100,\n  \"init_epochs\": 5,\n  \"batch_size\": 8,\n  \"device\": \"cuda\",\n  \"checkpoint_dir\": \"/kaggle/working/checkpoints\",\n  \"save_image_dir\": \"/kaggle/working/samples\",\n  \"lr_g\": 8e-5,\n  \"lr_d\": 16e-5,\n  \"init_lr\": 1e-3,\n  \"wadvg\": 30,\n  \"wadvd\": 30,\n  \"wcon\": 1.5,\n  \"wgra\": 3,\n  \"wcol\": 30,\n  \"resume\": \"GD\",\n  \"save_interval\": 1,\n  \"d_layers\": 3,\n  \"use_sn\": \"True\",\n  \"gan_loss\": \"lsgan\",\n  \"d_noise\": \"True\"\n}","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:42:56.094240Z","iopub.execute_input":"2024-01-05T21:42:56.094898Z","iopub.status.idle":"2024-01-05T21:42:56.100622Z","shell.execute_reply.started":"2024-01-05T21:42:56.094865Z","shell.execute_reply":"2024-01-05T21:42:56.099548Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"main(json_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:39:10.009628Z","iopub.execute_input":"2023-12-30T14:39:10.010448Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:02<00:00, 256MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"Compute mean (R, G, B) from 1792 images\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1792/1792 [00:14<00:00, 127.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"Mean(B, G, R) of /kaggle/input/anime-dataset/Hayao are [-4.4346958  -8.66591597 13.10061177]\nDataset: real 6656 style 1792, smooth 1792\nG weight loaded\nD weight loaded\nEpoch 27/100\n","output_type":"stream"},{"name":"stderr","text":"loss G: adv 9.973940 con 3.640771 gram 0.021225 color 1.734808 / loss D: 0.299507: 100%|██████████| 1110/1110 [31:05<00:00,  1.68s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 28/100\n","output_type":"stream"},{"name":"stderr","text":"loss G: adv 9.946950 con 3.571757 gram 0.021154 color 1.691009 / loss D: 0.247553: 100%|██████████| 1110/1110 [30:24<00:00,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 29/100\n","output_type":"stream"},{"name":"stderr","text":"loss G: adv 9.951957 con 3.539820 gram 0.021132 color 1.667478 / loss D: 0.289328: 100%|██████████| 1110/1110 [30:24<00:00,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 30/100\n","output_type":"stream"},{"name":"stderr","text":"loss G: adv 9.981235 con 3.449651 gram 0.021264 color 1.623905 / loss D: 0.232718: 100%|██████████| 1110/1110 [30:24<00:00,  1.64s/it]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 31/100\n","output_type":"stream"},{"name":"stderr","text":"loss G: adv 9.972983 con 3.454667 gram 0.021285 color 1.609291 / loss D: 0.264187:  67%|██████▋   | 748/1110 [20:29<09:56,  1.65s/it]","output_type":"stream"}]},{"cell_type":"code","source":"os.listdir('/kaggle/working/checkpoints')","metadata":{"execution":{"iopub.status.busy":"2023-12-25T13:59:19.252027Z","iopub.execute_input":"2023-12-25T13:59:19.252441Z","iopub.status.idle":"2023-12-25T13:59:19.261241Z","shell.execute_reply.started":"2023-12-25T13:59:19.252408Z","shell.execute_reply":"2023-12-25T13:59:19.260221Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"['generator_Hayao.pth', 'discriminator_Hayao.pth', 'generator_Hayao_init.pth']"},"metadata":{}}]},{"cell_type":"code","source":"!zip -r /kaggle/working/checkpoints-51.zip /kaggle/working/checkpoints","metadata":{"execution":{"iopub.status.busy":"2023-12-25T13:59:22.887727Z","iopub.execute_input":"2023-12-25T13:59:22.888516Z","iopub.status.idle":"2023-12-25T14:00:06.864646Z","shell.execute_reply.started":"2023-12-25T13:59:22.888480Z","shell.execute_reply":"2023-12-25T14:00:06.863472Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"  adding: kaggle/working/checkpoints/ (stored 0%)\n  adding: kaggle/working/checkpoints/generator_Hayao.pth (deflated 8%)\n  adding: kaggle/working/checkpoints/discriminator_Hayao.pth (deflated 8%)\n  adding: kaggle/working/checkpoints/generator_Hayao_init.pth (deflated 9%)\n","output_type":"stream"}]},{"cell_type":"code","source":"from IPython.display import FileLink","metadata":{"execution":{"iopub.status.busy":"2023-12-25T14:00:06.867164Z","iopub.execute_input":"2023-12-25T14:00:06.867592Z","iopub.status.idle":"2023-12-25T14:00:06.873165Z","shell.execute_reply.started":"2023-12-25T14:00:06.867548Z","shell.execute_reply":"2023-12-25T14:00:06.872125Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"%cd /kaggle/working","metadata":{"execution":{"iopub.status.busy":"2023-12-25T14:00:06.874572Z","iopub.execute_input":"2023-12-25T14:00:06.874930Z","iopub.status.idle":"2023-12-25T14:00:06.886049Z","shell.execute_reply.started":"2023-12-25T14:00:06.874894Z","shell.execute_reply":"2023-12-25T14:00:06.885208Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"/kaggle/working\n","output_type":"stream"}]},{"cell_type":"code","source":"FileLink(r'checkpoints-51.zip')","metadata":{"execution":{"iopub.status.busy":"2023-12-25T14:00:06.887951Z","iopub.execute_input":"2023-12-25T14:00:06.888203Z","iopub.status.idle":"2023-12-25T14:00:06.898197Z","shell.execute_reply.started":"2023-12-25T14:00:06.888180Z","shell.execute_reply":"2023-12-25T14:00:06.897360Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/checkpoints-40.zip","text/html":"<a href='checkpoints-40.zip' target='_blank'>checkpoints-40.zip</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"def input_transform(image_path):\n    image = cv2.imread(image_path)[:,:,::-1]\n    image = image.astype(np.float32)\n    image = normalize_input(image)\n    image = image.transpose(2, 0, 1)\n    return torch.tensor(image)","metadata":{"execution":{"iopub.status.busy":"2024-01-05T21:43:01.616185Z","iopub.execute_input":"2024-01-05T21:43:01.617007Z","iopub.status.idle":"2024-01-05T21:43:01.622002Z","shell.execute_reply.started":"2024-01-05T21:43:01.616969Z","shell.execute_reply":"2024-01-05T21:43:01.621058Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"test_generator = Generator(args.dataset)\ncheckpoint = torch.load(weight,  map_location='cuda:0') if torch.cuda.is_available() else \\\n    torch.load(weight,  map_location='cpu')\nmodel.load_state_dict(checkpoint['model_state_dict'], strict=True)\nepoch = checkpoint['epoch']\ndel checkpoint\ntorch.cuda.empty_cache()\ngc.collect()\ngenerator.eval()\nmax_iter = (max_imgs // args['batch_size']) + 1\nfake_imgs = []\nfor i, (img, *_) in enumerate(loader):\n    with torch.no_grad():\n        fake_img = generator(img.cuda())\n        fake_img = fake_img.detach().cpu().numpy()\n        # Channel first -> channel last\n        fake_img = fake_img.transpose(0, 2, 3, 1)\n        fake_imgs.append(denormalize_input(fake_img, dtype=np.int16))\n    if i + 1 == max_iter:\n        break\nfake_imgs = np.concatenate(fake_imgs, axis=0)\nfor i, img in enumerate(fake_imgs):\n    save_path = os.path.join(args['save_image_dir'], f'{subname}_{i}.jpg')\n    cv2.imwrite(save_path, img[..., ::-1])","metadata":{},"execution_count":null,"outputs":[]}]}